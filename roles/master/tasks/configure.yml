---
# This playbook configures the services

###########################
# Tailscale Configuration #
###########################

- name: Configure Tailscale in profile for sudoers
  lineinfile:
    path: /etc/profile
    state: present
    line: |
      if [ "$(id -u)" -eq 0 ] || sudo -v &>/dev/null; then
        if ! tailscale status &>/dev/null; then
          echo "######################################################################"
          echo "#  Tailscale SSH enables secure external access to the system via    #"
          echo "#  the Tailscale network. All other SSH connections are disabled.    #"
          echo "#  For more info: https://tailscale.com/kb/1193/tailscale-ssh        #"
          echo "######################################################################"
          sudo tailscale up --ssh
        fi
      fi
  tags: tailscale-config

##########################
# Firewall Configuration #
##########################

- name: Start firewalld service
  systemd:
    name: firewalld
    state: started
  tags: firewalld-config

- name: Set default zone to "public" only if necessary
  command: firewall-cmd --set-default-zone=public
  ignore_errors: true
  tags: firewalld-config

- name: Block all incoming traffic by default, allowing established connections out
  command: firewall-cmd --permanent --zone=public --set-target=DROP
  tags: firewalld-config

- name: Create a tailscale zone for Tailscale
  command: firewall-cmd --permanent --new-zone=tailscale
  tags: firewalld-config

- name: Allow tailscale zone to accept traffic from Tailscale
  command: firewall-cmd --zone=tailscale --add-interface=tailscale0 --permanent
  tags: firewalld-config

- name: Create a cilium zone for Cilium to do DNS resolution
  command: firewall-cmd --permanent --new-zone=cilium
  tags: firewalld-config

- name: Allow cilium zone to accept traffic from Cilium
  command: firewall-cmd --zone=cilium --add-interface=cilium_host --permanent
  tags: firewalld-config

- name: Allow cilium zone to masquerade
  command: firewall-cmd --zone=cilium --add-masquerade --permanent
  tags: firewalld-config

- name: Allow cilium zone to accept DNS
  command: firewall-cmd --zone=cilium --add-service=dns --permanent
  notify:
    - Restart firewalld service
  tags: firewalld-config

#######################
# Slurm Configuration #
#######################

- name: Ensure Slurm's controller state directory exists
  file:
    path: "{{ master_slurm.state_save_location }}"
    owner: slurm
    group: slurm
    mode: 0755
    state: directory
  tags: slurm-config

- name: Generate a Munge Key
  # This is usually a no-op as the package install actually generates a (node-unique) one, so won't usually trigger handler.
  command: "dd if=/dev/urandom of=/etc/munge/munge.key bs=1 count=1024"
  args:
    creates: "/etc/munge/munge.key"
  tags: slurm-config

- name: Fix permissions on /etc to pass Munge startup checks
  # Rocky-9-GenericCloud-Base-9.4-20240523.0.x86_64.qcow2 makes /etc g=rwx rather than g=rx (where group=root)
  file:
    path: /etc
    state: directory
    mode: g-w
  when: ansible_distribution == 'Rocky' and ansible_distribution_major_version == '9'
  tags: slurm-config

- name: Install LuaSocket and AMQP libraries via LuaRocks
  ansible.builtin.command: luarocks install {{ item }}
  loop:
    - luasocket
    - lua-amqp
  tags: slurm-config

- name: Create the Lua script for job completion
  ansible.builtin.copy:
    dest: "/etc/slurm/jobcomp.lua"
    mode: '0755'
    content: |
      local amqp = require("amqp")

      function slurm_jobcomp_process(job_table)
          local job_id = job_table["job_id"]
          local job_name = job_table["job_name"]
          local job_state = job_table["job_state"]
          local user_id = job_table["user_id"]
          local partition = job_table["partition"]
          local submit_time = job_table["submit_time"]

          -- Create the JSON payload to send to RabbitMQ
          local job_payload = string.format(
            '{"job_id": "%s", "job_name": "%s", "job_state": "%s", "user_id": "%s", "partition": "%s", "submit_time": "%s"}',
            job_id, job_name, job_state, user_id, partition, submit_time
          )

          -- Connect to RabbitMQ
          local conn = amqp.Connection:new({host = "localhost", port = 5672})
          conn:connect()
          conn:open_channel()

          -- Declare the queue if it doesn't exist
          conn:declare_queue("slurm_jobs")

          -- Publish the message to the queue
          conn:publish_message(job_payload, "slurm_jobs")

          -- Close the connection
          conn:close()

          slurm.log_info("Job completion data sent to RabbitMQ: " .. job_payload)
          return slurm.SUCCESS
      end
  tags: slurm-config

- name: Template basic slurm.conf
  template:
    src: slurm.conf.j2
    dest: /etc/slurm/slurm.conf
    lstrip_blocks: true
    owner: root
    group: root
    mode: 0644
  notify:
    - Restart slurmctld service
  changed_when: false
  become: false
  tags: slurm-config

- name: Create gres.conf
  template:
    src: "gres.conf.j2"
    dest: /etc/slurm/gres.conf
    mode: "0600"
    owner: slurm
    group: slurm
  notify:
    - Restart slurmctld service
  register: ohpc_gres_conf
  changed_when: false
  become: false
  tags: slurm-config

- name: Template cgroup.conf
  # Appears to be required even with NO cgroup plugins: https://slurm.schedmd.com/cgroups.html#cgroup_design
  template:
    src: cgroup.conf.j2
    dest: /etc/slurm/cgroup.conf
    mode: "0644"
    owner: root
    group: root
  changed_when: false
  become: false
  tags: slurm-config

########################
# ArgoCD Configuration #
########################

- name: Create ArgoCD ApplicationSet
  copy:
    content: |
      apiVersion: argoproj.io/v1alpha1
      kind: ApplicationSet
      metadata:
        name: all-apps
        namespace: argocd
      spec:
        generators:
          - list:
              elements:
                - name: bgpdata
                  repoURL: https://github.com/bgpdata/bgpdata.git
                  chartPath: charts/bgpdata
                - name: ris-kafka
                  repoURL: https://github.com/robin-rpr/ris-kafka.git
                  chartPath: charts/ris-kafka
                - name: cernide
                  repoURL: https://github.com/cernide/cernide.git
                  chartPath: charts/polyaxon
        template:
          metadata:
            name: '{% raw %}{{name}}{% endraw %}'
          spec:
            project: default
            source:
              repoURL: '{% raw %}{{repoURL}}{% endraw %}'
              targetRevision: HEAD
              path: '{% raw %}{{chartPath}}{% endraw %}'
              helm:
                valueFiles:
                  - values-ci.yaml
            destination:
              server: https://kubernetes.default.svc
              namespace: '{% raw %}{{name}}{% endraw %}'
            syncPolicy:
              automated:
                prune: true
                selfHeal: true
              syncOptions:
                - CreateNamespace=true
    dest: /tmp/applicationset.yaml
    mode: '0644'
  tags: argocd-config

- name: Apply ArgoCD ApplicationSet
  command: kubectl apply -f /tmp/applicationset.yaml
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
    PATH: "{{ ansible_env.PATH }}:/usr/local/bin"
  register: argocd_result
  changed_when: argocd_result.stdout is search("created|configured|updated")
  tags: argocd-config

#########################
# MetalLB Configuration #
#########################

- name: Create MetalLB config
  copy:
    content: |
      apiVersion: v1
      kind: ConfigMap
      metadata:
        namespace: metallb-system
        name: config
      data:
        config: |
          address-pools:
          - name: ipv4-pool
            protocol: layer2
            addresses:
            - {{ master_network.ip_pool }}
          - name: ipv6-pool
            protocol: layer2
            addresses:
            - {{ master_network.ip_v6_pool }}
    dest: /tmp/metallb-config.yaml
    mode: '0644'
  tags: metallb-config

- name: Apply MetalLB config
  command: kubectl apply -f /tmp/metallb-config.yaml
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
    PATH: "{{ ansible_env.PATH }}:/usr/local/bin"
  register: metallb_result
  changed_when: metallb_result.stdout is search("created|configured|updated")
  tags: metallb-config

##############################
# Cert-Manager Configuration #
##############################

- name: Apply Let's Encrypt ClusterIssuer
  copy:
    dest: /tmp/letsencrypt-cluster-issuer.yaml
    content: |
      apiVersion: cert-manager.io/v1
      kind: ClusterIssuer
      metadata:
        name: letsencrypt-prod
        namespace: cert-manager
      spec:
        acme:
          server: https://acme-v02.api.letsencrypt.org/directory
          email: {{ master_kubernetes.email }}
          privateKeySecretRef:
            name: letsencrypt-prod
          solvers:
          - http01:
              ingress:
                class: traefik
  tags: certmanager-config

############################
# Kubernetes Configuration #
############################

- name: Ensure /etc/rancher/k3s/k3s.yaml has 600 permissions
  file:
    path: /etc/rancher/k3s/k3s.yaml
    mode: '0600'
    state: file
  become: true
  tags: kubernetes-config